{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(scripts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Chinese MNIST using Transfer Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = 15\n",
    "IMAGE_SIZE = (64, 64)\n",
    "IMAGE_SHAPE = IMAGE_SIZE + (3,)\n",
    "\n",
    "CLASSIFIER_LEARNING_RATE = 0.0001\n",
    "CLASSFIER_NUMBER_OF_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "OPTIMIZER = tf.keras.optimizers.Adam\n",
    "LOSS_MEASURE = tf.keras.losses.SparseCategoricalCrossentropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_training_dataloader = tf.keras.utils.image_dataset_from_directory(\"./small_training_dataset/\",\n",
    "                                                                        batch_size=BATCH_SIZE,\n",
    "                                                                        image_size=IMAGE_SIZE,\n",
    "                                                                        seed=413)\n",
    "medium_training_dataloader = tf.keras.utils.image_dataset_from_directory(\"./medium_training_dataset/\",\n",
    "                                                                         batch_size=BATCH_SIZE,\n",
    "                                                                         image_size=IMAGE_SIZE,\n",
    "                                                                         seed=413)\n",
    "large_training_dataloader = tf.keras.utils.image_dataset_from_directory(\"./large_training_dataset/\",\n",
    "                                                                        batch_size=BATCH_SIZE,\n",
    "                                                                        image_size=IMAGE_SIZE,\n",
    "                                                                        seed=413)\n",
    "validation_dataloader = tf.keras.utils.image_dataset_from_directory(\"./validation_dataset/\",\n",
    "                                                                    batch_size=BATCH_SIZE,\n",
    "                                                                    image_size=IMAGE_SIZE,\n",
    "                                                                    seed=413)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a Peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts.peek_into_dataloader(small_training_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_training_dataloader = small_training_dataloader.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "medium_training_dataloader = medium_training_dataloader.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "large_training_dataloader = large_training_dataloader.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_dataloader = validation_dataloader.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_efficientnet_base_model = tf.keras.applications.EfficientNetV2S(include_top=False, input_shape=IMAGE_SHAPE) # expects an input to be in the range [0, 255].\n",
    "medium_efficientnet_base_model = tf.keras.applications.EfficientNetV2M(include_top=False, input_shape=IMAGE_SHAPE)\n",
    "large_efficientnet_base_model = tf.keras.applications.EfficientNetV2L(include_top=False, input_shape=IMAGE_SHAPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_efficientnet = scripts.add_classifier(small_efficientnet_base_model, IMAGE_SHAPE, NUMBER_OF_CLASSES)\n",
    "medium_efficientnet = scripts.add_classifier(medium_efficientnet_base_model, IMAGE_SHAPE, NUMBER_OF_CLASSES)\n",
    "large_efficientnet = scripts.add_classifier(large_efficientnet_base_model, IMAGE_SHAPE, NUMBER_OF_CLASSES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of layers in the small efficientnet base model: {}\".format(len(small_efficientnet.layers[1].layers)))\n",
    "print(\"Number of layers in the medium efficientnet base model: {}\".format(len(medium_efficientnet.layers[1].layers)))\n",
    "print(\"Number of layers in the large efficientnet base model: {}\".format(len(large_efficientnet.layers[1].layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_efficientnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_efficientnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_efficientnet.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_NAMES = np.array([[\"small_efficientnet_with_small_dataset\", \"small_efficientnet_with_medium_dataset\", \"small_efficientnet_with_large_dataset\"],\n",
    "                             [\"medium_efficientnet_with_small_dataset\", \"medium_efficientnet_with_medium_dataset\", \"medium_efficientnet_with_large_dataset\"],\n",
    "                             [\"large_efficientnet_with_small_dataset\", \"large_efficientnet_with_medium_dataset\", \"large_efficientnet_with_large_dataset\"]])\n",
    "MODELS = (small_efficientnet, medium_efficientnet, large_efficientnet)\n",
    "DATALOADERS = (small_training_dataloader, medium_training_dataloader, large_training_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_after_training_classifiers = scripts.train_classifiers(MODELS,\n",
    "                                                                  DATALOADERS,\n",
    "                                                                  validation_dataloader,\n",
    "                                                                  OPTIMIZER(learning_rate=CLASSIFIER_LEARNING_RATE),\n",
    "                                                                  LOSS_MEASURE(from_logits=True),\n",
    "                                                                  CLASSFIER_NUMBER_OF_EPOCHS,\n",
    "                                                                  CHECKPOINT_NAMES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "Only fine tune after the classifer have been trained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINE_TUNING_LEARNING_RATE = CLASSIFIER_LEARNING_RATE / 10\n",
    "FINE_TUNING_NUMBER_OF_EPOCHS = 10\n",
    "TOTAL_NUMBER_OF_EPOCHS = CLASSFIER_NUMBER_OF_EPOCHS + FINE_TUNING_NUMBER_OF_EPOCHS\n",
    "PERCENTAGE_OF_LAYERS_TO_FREEZE = [0.2, 0.4, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts.fine_tune(MODELS, DATALOADERS, validation_dataloader, OPTIMIZER(learning_rate=CLASSIFIER_LEARNING_RATE), LOSS_MEASURE(from_logits=True), TOTAL_NUMBER_OF_EPOCHS, CHECKPOINT_NAMES, PERCENTAGE_OF_LAYERS_TO_FREEZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc413",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
